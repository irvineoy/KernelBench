// Optimized Conv2d (stride=1, padding=0, dilation=1) for AMD MI300X (gfx942)
// Single-file HIP kernel + PyTorch binding
// Supports grouping inferred from weight shape. Bias not included (model sets bias=False).
// Author: Generated by AI

#include <hip/hip_runtime.h>
#include <torch/extension.h>
#include <iostream>

#ifndef TILE_H
#define TILE_H 16
#endif

#ifndef TILE_W
#define TILE_W 16
#endif

#ifndef C_TILE
#define C_TILE 8
#endif

#ifndef MAX_K
#define MAX_K 11  // Shared-memory path supports kernels up to this size
#endif

// Tiled shared-memory kernel for Conv2d with stride=1, pad=0, dil=1
// Grid: (ceil(OW/TILE_W), ceil(OH/TILE_H), N * OC)
// Block: (TILE_W, TILE_H)
__global__ void conv2d_tiled_kernel(
    const float* __restrict__ input,   // [N, C_in, H, W]
    const float* __restrict__ weight,  // [OC, C_in/groups, K, K]
    float* __restrict__ output,        // [N, OC, OH, OW]
    int N, int C_in, int H, int W,
    int OC, int K, int groups,
    int OH, int OW) {

    // Early exit if nothing to do
    if (OH <= 0 || OW <= 0) return;

    // Map block to output tile
    const int tx = threadIdx.x; // [0, TILE_W)
    const int ty = threadIdx.y; // [0, TILE_H)
    const int bx = blockIdx.x;
    const int by = blockIdx.y;
    const int bz = blockIdx.z;

    const int oc = bz % OC;
    const int b  = bz / OC;

    const int out_h0 = by * TILE_H;
    const int out_w0 = bx * TILE_W;

    if (out_h0 >= OH || out_w0 >= OW || b >= N || oc >= OC) return;

    const int effOH = min(TILE_H, OH - out_h0);
    const int effOW = min(TILE_W, OW - out_w0);

    // Group inference
    const int oc_per_group = OC / max(1, groups);
    const int cin_per_group = C_in / max(1, groups);
    const int g = oc / oc_per_group;
    const int cin_group_start = g * cin_per_group;

    // Shared memory tiles (+1 pad on fastest dim to mitigate bank conflicts)
    __shared__ float s_input[C_TILE][TILE_H + MAX_K - 1][TILE_W + MAX_K - 1 + 1];
    __shared__ float s_weight[C_TILE][MAX_K][MAX_K];

    float acc = 0.0f;

    const int block_threads = blockDim.x * blockDim.y;
    const int tid = threadIdx.y * blockDim.x + threadIdx.x;

    // Iterate over input channels in chunks
    for (int c0 = 0; c0 < cin_per_group; c0 += C_TILE) {
        const int c_eff = min(C_TILE, cin_per_group - c0);

        // Load weights for current (oc, c-chunk) into LDS
        // weight layout: [OC, cin_per_group, K, K]
        {
            const int total_w = c_eff * K * K;
            for (int idx = tid; idx < total_w; idx += block_threads) {
                int kk = idx;
                const int kw = kk % K; kk /= K;
                const int kh = kk % K; kk /= K;
                const int c_rel = kk;  // [0, c_eff)

                const int c_in_rel = c0 + c_rel;
                const int w_index = (((oc * cin_per_group) + c_in_rel) * K + kh) * K + kw;  // flattened
                s_weight[c_rel][kh][kw] = weight[w_index];
            }
        }

        // Load input tile for current c-chunk into LDS
        // For stride=1,pad=0,dil=1: input tile origin is (in_h0, in_w0) = (out_h0, out_w0)
        const int in_h0 = out_h0;
        const int in_w0 = out_w0;
        const int tileHpad = effOH + K - 1;
        const int tileWpad = effOW + K - 1;

        for (int c_rel = 0; c_rel < c_eff; ++c_rel) {
            const int c_in = cin_group_start + (c0 + c_rel);
            const int plane_stride = H * W;
            const int batch_offset = b * (C_in * plane_stride);
            const int chan_offset = c_in * plane_stride;

            const int total_elems = tileHpad * tileWpad;
            for (int idx = tid; idx < total_elems; idx += block_threads) {
                const int y = idx / tileWpad;
                const int x = idx - y * tileWpad;
                const int ih = in_h0 + y;
                const int iw = in_w0 + x;
                // Within bounds due to effOH/effOW selection
                const int in_index = batch_offset + chan_offset + ih * W + iw;
                s_input[c_rel][y][x] = input[in_index];
            }
        }

        __syncthreads();

        // Compute output for this (b, oc) tile and channel chunk
        if (tx < effOW && ty < effOH) {
            // pointer into shared input tile for current output pixel
            const int in_y = ty;
            const int in_x = tx;

            // Accumulate over c_eff and KxK
            #pragma unroll 1
            for (int c_rel = 0; c_rel < c_eff; ++c_rel) {
                #pragma unroll 1
                for (int kh = 0; kh < K; ++kh) {
                    #pragma unroll 1
                    for (int kw = 0; kw < K; ++kw) {
                        float v = s_input[c_rel][in_y + kh][in_x + kw];
                        float wv = s_weight[c_rel][kh][kw];
                        acc += v * wv;
                    }
                }
            }
        }

        __syncthreads();
    }

    // Store result
    if (tx < effOW && ty < effOH) {
        const int oh = out_h0 + ty;
        const int ow = out_w0 + tx;
        const int out_index = ((b * OC + oc) * OH + oh) * OW + ow;
        output[out_index] = acc;
    }
}

// Naive fallback kernel (no shared memory). Works for any K.
__global__ void conv2d_naive_kernel(
    const float* __restrict__ input,   // [N, C_in, H, W]
    const float* __restrict__ weight,  // [OC, C_in/groups, K, K]
    float* __restrict__ output,        // [N, OC, OH, OW]
    int N, int C_in, int H, int W,
    int OC, int K, int groups,
    int OH, int OW) {

    if (OH <= 0 || OW <= 0) return;

    const int tx = threadIdx.x;
    const int ty = threadIdx.y;
    const int bx = blockIdx.x;
    const int by = blockIdx.y;
    const int bz = blockIdx.z;

    const int oc = bz % OC;
    const int b  = bz / OC;

    const int oh = by * blockDim.y + ty;
    const int ow = bx * blockDim.x + tx;

    if (oh >= OH || ow >= OW || b >= N || oc >= OC) return;

    // Group inference
    const int oc_per_group = OC / max(1, groups);
    const int cin_per_group = C_in / max(1, groups);
    const int g = oc / oc_per_group;
    const int cin_group_start = g * cin_per_group;

    float acc = 0.0f;

    // Input origin for this output position (stride=1, pad=0)
    const int in_h0 = oh;
    const int in_w0 = ow;

    for (int c_rel = 0; c_rel < cin_per_group; ++c_rel) {
        const int c_in = cin_group_start + c_rel;

        for (int kh = 0; kh < K; ++kh) {
            const int ih = in_h0 + kh;
            const int base_in = ((b * C_in + c_in) * H + ih) * W + in_w0;

            for (int kw = 0; kw < K; ++kw) {
                const int iw = in_w0 + kw;
                const int in_index = base_in + kw; // contiguous in kw
                const int w_index  = (((oc * cin_per_group) + c_rel) * K + kh) * K + kw;
                acc += input[in_index] * weight[w_index];
            }
        }
    }

    const int out_index = ((b * OC + oc) * OH + oh) * OW + ow;
    output[out_index] = acc;
}

// PyTorch-visible entry point: only tensor parameters (input, weight)
// Assumes stride=1, padding=0, dilation=1 (as in the provided model)
// Groups inferred from weight shape: groups = C_in / weight.size(1)
at::Tensor run(at::Tensor input, at::Tensor weight) {
    TORCH_CHECK(input.is_cuda(), "Input must be a CUDA/HIP tensor");
    TORCH_CHECK(weight.is_cuda(), "Weight must be a CUDA/HIP tensor");
    TORCH_CHECK(input.dtype() == at::kFloat, "Only float32 supported");
    TORCH_CHECK(weight.dtype() == at::kFloat, "Only float32 weight supported");
    TORCH_CHECK(input.dim() == 4, "Input must be NCHW");
    TORCH_CHECK(weight.dim() == 4, "Weight must be OIHW (grouped O, I_per_group, H, W)");

    auto x = input.contiguous();
    auto w = weight.contiguous();

    const int N  = x.size(0);
    const int C  = x.size(1);
    const int H  = x.size(2);
    const int W  = x.size(3);

    const int OC = w.size(0);
    const int Ipg = w.size(1);     // in_channels per group
    const int K   = w.size(2);
    TORCH_CHECK(w.size(3) == K, "Kernel must be square");

    TORCH_CHECK(K >= 1, "Kernel size must be >= 1");
    TORCH_CHECK(C % Ipg == 0, "Input channels must be divisible by weight.size(1)");
    const int groups = C / Ipg;
    TORCH_CHECK(OC % groups == 0, "Out channels must be divisible by groups");

    // stride=1, padding=0, dilation=1
    const int OH = H - (K - 1) - 1 + 1;  // H - K + 1
    const int OW = W - (K - 1) - 1 + 1;  // W - K + 1
    TORCH_CHECK(OH >= 0 && OW >= 0, "Invalid output size; kernel larger than input without padding");

    auto y = at::empty({N, OC, OH, OW}, x.options());

    // Launch configuration
    dim3 block(TILE_W, TILE_H, 1);
    dim3 grid((OW + TILE_W - 1) / TILE_W,
              (OH + TILE_H - 1) / TILE_H,
              N * OC);

    // Choose kernel: use tiled shared-memory path when K <= MAX_K; otherwise naive fallback
    if (K <= MAX_K) {
        hipLaunchKernelGGL(
            conv2d_tiled_kernel,
            grid, block, 0, 0,
            x.data_ptr<float>(),
            w.data_ptr<float>(),
            y.data_ptr<float>(),
            N, C, H, W,
            OC, K, groups, OH, OW
        );
    } else {
        hipLaunchKernelGGL(
            conv2d_naive_kernel,
            grid, block, 0, 0,
            x.data_ptr<float>(),
            w.data_ptr<float>(),
            y.data_ptr<float>(),
            N, C, H, W,
            OC, K, groups, OH, OW
        );
    }

    hipError_t err = hipDeviceSynchronize();
    TORCH_CHECK(err == hipSuccess, "HIP kernel sync failed: ", hipGetErrorString(err));
    err = hipGetLastError();
    TORCH_CHECK(err == hipSuccess, "HIP kernel launch failed: ", hipGetErrorString(err));

    return y;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("run", &run, "Optimized Conv2d (stride=1, pad=0, dil=1) with groups inferred (HIP)");
}