# LLM Service Configuration
# Supports: OpenAI GPT-4o/GPT-4, Claude Opus 4/Sonnet 4.5, OpenRouter, and vLLM local models

# Provider selection: openai, claude, openrouter, or vllm
provider: claude

# OpenAI Configuration (GPT-5)
openai:
  api_key: ${OPENAI_API_KEY}  # or set directly
  base_url: https://api.openai.com/v1
  model: gpt-5  # Options: gpt-5
  effort: low  # For GPT-5: low, medium, high
  max_tokens: 272_000  # For GPT-5 this maps to max_output_tokens, for others it's max_tokens
  timeout: 1800

# Claude Configuration (Opus 4, Sonnet 4.5)
claude:
  api_key: ${ANTHROPIC_API_KEY}  # or set directly
  base_url: https://api.anthropic.com
  model: claude-sonnet-4-5-20250929  # Options: claude-opus-4-1-20250805, claude-sonnet-4-5-20250929
  temperature: 0.1
  max_tokens: 64000
  timeout: 1000
  thinking: enabled  # Enable thinking process output (enabled/disabled, default: disabled)
  thinking_budget: 4096  # Budget tokens for thinking when enabled (minimum 1024)

# OpenRouter Configuration (o4-mini, o4-mini-7b-preview)
openrouter:
  api_key: ${OPENROUTER_API_KEY}  # or set directly
  base_url: https://openrouter.ai
  model: qwen/qwen3-next-80b-a3b-instruct  # Options: openai/o4-mini, openai/o4-mini-7b-preview
  effort: high  # Reasoning effort: minimal, low, medium, high
  max_output_tokens: 64000  # Max output tokens for reasoning models
  temperature: 0.1  # Temperature (optional, often not used with reasoning models)
  timeout: 1800

# vLLM Local Model Configuration
vllm:
  base_url: http://localhost:30001/v1  # vLLM server endpoint
  model: llamas_team_local_llm  # Model name on vLLM server
  api_key: dummy  # API key as configured in vLLM server (--api-key dummy)
  temperature: 0.1
  max_tokens: 128_000
  timeout: 300

# Retry configuration (applies to all providers)
retry:
  max_retries: 3
  retry_delay: 2  # seconds
  exponential_backoff: true
