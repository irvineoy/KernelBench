# LLM Service Configuration
# Supports: OpenAI GPT-4o/GPT-4, Claude Opus 4/Sonnet 4.5, and vLLM local models

# Provider selection: openai, claude, or vllm
provider: claude

# OpenAI Configuration (GPT-5)
openai:
  api_key: ${OPENAI_API_KEY}  # or set directly
  base_url: https://api.openai.com/v1
  model: gpt-5  # Options: gpt-5
  effort: low  # For GPT-5: low, medium, high
  max_tokens: 272_000  # For GPT-5 this maps to max_output_tokens, for others it's max_tokens
  timeout: 1800

# Claude Configuration (Opus 4, Sonnet 4.5)
claude:
  api_key: ${ANTHROPIC_API_KEY}  # or set directly
  base_url: https://api.anthropic.com
  model: claude-sonnet-4-5-20250929  # Options: claude-opus-4-20250514, claude-sonnet-4-5-20250929
  temperature: 0.1
  max_tokens: 64000
  timeout: 1000

# vLLM Local Model Configuration
vllm:
  base_url: http://localhost:30001/v1  # vLLM server endpoint
  model: llamas_team_local_llm  # Model name on vLLM server
  api_key: dummy  # API key as configured in vLLM server (--api-key dummy)
  temperature: 0.1
  max_tokens: 128_000
  timeout: 300

# Retry configuration (applies to all providers)
retry:
  max_retries: 3
  retry_delay: 2  # seconds
  exponential_backoff: true
